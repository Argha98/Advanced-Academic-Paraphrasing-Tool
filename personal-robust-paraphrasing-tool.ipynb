{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Academic Paraphrasing Tool**\n",
    "=======================================\n",
    "\n",
    "üìã¬†**Project Documentation**\n",
    "----------------------------\n",
    "\n",
    "**1\\. Project Overview**\n",
    "------------------------\n",
    "\n",
    "**Title**\n",
    "---------\n",
    "\n",
    "Humanized LLM-Powered Academic Paraphrasing System with Quality Assessment\n",
    "\n",
    "**Description**\n",
    "---------------\n",
    "\n",
    "An advanced paraphrasing tool designed for academic research that combines large language models (Llama 3.3 70B) with humanization post-processing to generate high-quality, plagiarism-free paraphrases while preserving original meaning and technical accuracy.\n",
    "\n",
    "**Version**\n",
    "-----------\n",
    "\n",
    "1.0.0 (February 2026)\n",
    "\n",
    "**Author**\n",
    "----------\n",
    "\n",
    "Argha Mukherjee - Computer Science & Machine Learning\n",
    "\n",
    "**Repository Type**\n",
    "-------------------\n",
    "\n",
    "Kaggle Notebook (GPU-accelerated)\n",
    "\n",
    "**2\\. Objective**\n",
    "-----------------\n",
    "\n",
    "**Primary Objectives**\n",
    "----------------------\n",
    "\n",
    "1.  **Preserve Original Meaning**: Generate paraphrases that maintain 78-92% semantic similarity to source text\n",
    "    \n",
    "2.  **Avoid Plagiarism**: Ensure 30-55% lexical overlap (40%+ word variation from original)\n",
    "    \n",
    "3.  **Humanize Output**: Remove AI-generated artifacts and robotic phrasing to produce natural, human-like text\n",
    "    \n",
    "4.  **Multi-Style Generation**: Provide three distinct paraphrasing styles (Academic, Concise, Technical)\n",
    "    \n",
    "5.  **Quality Assessment**: Real-time metrics for lexical overlap, semantic similarity, and humanness scores\n",
    "    \n",
    "\n",
    "**Secondary Objectives**\n",
    "------------------------\n",
    "\n",
    "*   Protect technical elements (equations, citations, URLs) during paraphrasing\n",
    "    \n",
    "*   Provide batch processing capabilities for multiple paragraphs\n",
    "    \n",
    "*   Generate reproducible results with source citation reminders\n",
    "    \n",
    "*   Enable offline/online hybrid operation (local embeddings + API models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéì Citation**\n",
    "---------------\n",
    "\n",
    "If you use this tool in your research or academic work, please cite:\n",
    "\n",
    "**BibTeX**\n",
    "----------\n",
    "\n",
    "@software{Personal ROBUST Paraphrasing Tool\\_2026,\n",
    "\n",
    "author = {\\[Argha Mukherjee\\]},\n",
    "\n",
    "title = {{Humanized LLM-Powered Academic Paraphrasing System with Quality Assessment}},\n",
    "\n",
    "year = {2026},\n",
    "\n",
    "month = feb,\n",
    "\n",
    "publisher = {Kaggle},\n",
    "\n",
    "institution = {Jadavpur University \\\\&\n",
    "School of Education Technology, Kolkata, India},\n",
    "\n",
    "howpublished = {\\\\url{[https://www.kaggle.com/code/arghamukherjee1998/personal-robust-paraphrasing-tool/](https://www.kaggle.com/code/arghamukherjee1998/personal-robust-paraphrasing-tool/)}},\n",
    "\n",
    "note = {Version 1.0.0. Developed as part of Post Graduate\n",
    "\n",
    "research in deep learning and NLP and for personal use.}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-03T22:38:23.107287Z",
     "iopub.status.busy": "2026-02-03T22:38:23.107002Z",
     "iopub.status.idle": "2026-02-03T22:38:23.380695Z",
     "shell.execute_reply": "2026-02-03T22:38:23.379913Z",
     "shell.execute_reply.started": "2026-02-03T22:38:23.107261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Disclaimer**\n",
    "-----------------\n",
    "\n",
    "This tool is designed to¬†**assist**¬†with academic writing, not replace critical thinking or original research. Users are responsible for:\n",
    "\n",
    "*   Verifying factual accuracy of paraphrased content\n",
    "    \n",
    "*   Properly citing all sources\n",
    "    \n",
    "*   Following institutional academic integrity policies\n",
    "    \n",
    "*   Ensuring compliance with journal/conference guidelines\n",
    "    \n",
    "*   Understanding the original material before paraphrasing\n",
    "    \n",
    "\n",
    "**The authors assume no liability for misuse or academic misconduct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:38:23.382484Z",
     "iopub.status.busy": "2026-02-03T22:38:23.382057Z",
     "iopub.status.idle": "2026-02-03T22:38:29.466616Z",
     "shell.execute_reply": "2026-02-03T22:38:29.465712Z",
     "shell.execute_reply.started": "2026-02-03T22:38:23.382451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h‚úÖ All dependencies installed!\n",
      "üìå Next: Get your free Groq API key from https://console.groq.com/keys\n"
     ]
    }
   ],
   "source": [
    "# CELL 1\n",
    "# CELL 1: Install Required Packages\n",
    "!pip install -q groq sentence-transformers torch nltk\n",
    "\n",
    "# Download WordNet for language processing\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")\n",
    "print(\"üìå Next: Get your free Groq API key from https://console.groq.com/keys\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Dependencies**\n",
    "----------------\n",
    "\n",
    "`   textgroq==0.4.2  sentence-transformers==3.3.1  torch==2.3.1  nltk==3.8.1  transformers==4.44.2  numpy==2.0.2  pandas==2.1.4   `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:38:29.468439Z",
     "iopub.status.busy": "2026-02-03T22:38:29.467790Z",
     "iopub.status.idle": "2026-02-03T22:39:02.937417Z",
     "shell.execute_reply": "2026-02-03T22:39:02.936662Z",
     "shell.execute_reply.started": "2026-02-03T22:38:29.468412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 22:38:42.375218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770158322.571630      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770158322.625166      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770158323.070367      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770158323.070402      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770158323.070405      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770158323.070407      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading semantic similarity model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef36ac8b21554e1ca843def1c0795d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8396444185514c7b8ac1a2e348defceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761eccb0b47642129dc465e1b851b740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11361edb58d0446eae54a595f33af8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ba13766eb847dfab2428ebbdc1c2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cdadb12bbb486b89548a5087977914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce595e80831e48629fdef303d391844d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b25c61f40846cc9512a9399a99285f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0367e3d619c44f56bd240587ccbae7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88261c982a794eb0a6471950228eb459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8faea740cbe445f7860a859cb0934bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported!\n",
      "‚úÖ Groq API configured with Llama 3.3 70B!\n",
      "‚úÖ Semantic analyzer loaded!\n",
      "üñ•Ô∏è  Device: CUDA\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Import Libraries and Configure Groq API\n",
    "\n",
    "import os\n",
    "from groq import Groq\n",
    "import re\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# üîë GROQ API KEY\n",
    "# ============================================================================\n",
    "GROQ_API_KEY = \"PASTE_YOUR_OWN_KEY_HERE\"\n",
    "# ============================================================================\n",
    "\n",
    "# Validate and initialize\n",
    "if GROQ_API_KEY == \"PASTE_YOUR_NEW_KEY_HERE\":\n",
    "    print(\"‚ùå ERROR: Please replace PASTE_YOUR_NEW_KEY_HERE with your actual Groq API key!\")\n",
    "    print(\"üìå Get it from: https://console.groq.com/keys\")\n",
    "else:\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "    \n",
    "    # Initialize Groq client\n",
    "    groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "    \n",
    "    # Load embedding model for quality analysis\n",
    "    print(\"‚è≥ Loading semantic similarity model...\")\n",
    "    embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    print(\"‚úÖ All libraries imported!\")\n",
    "    print(\"‚úÖ Groq API configured with Llama 3.3 70B!\")\n",
    "    print(f\"‚úÖ Semantic analyzer loaded!\")\n",
    "    print(f\"üñ•Ô∏è  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:39:02.940278Z",
     "iopub.status.busy": "2026-02-03T22:39:02.939618Z",
     "iopub.status.idle": "2026-02-03T22:39:02.951648Z",
     "shell.execute_reply": "2026-02-03T22:39:02.950818Z",
     "shell.execute_reply.started": "2026-02-03T22:39:02.940254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced token protector loaded!\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Advanced Token Protector\n",
    "\n",
    "class AdvancedTokenProtector:\n",
    "    \"\"\"Enhanced protection for scientific text elements\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'math_inline': re.compile(r'\\$([^\\$]+)\\$'),\n",
    "            'math_display': re.compile(r'\\\\\\[(.+?)\\\\\\]', re.DOTALL),\n",
    "            'latex_env': re.compile(r'\\\\begin\\{(\\w+)\\}(.*?)\\\\end\\{\\1\\}', re.DOTALL),\n",
    "            'code_inline': re.compile(r'`([^`]+)`'),\n",
    "            'code_block': re.compile(r'```[\\s\\S]*?```'),\n",
    "            'citation_latex': re.compile(r'\\\\cite\\{[^}]+\\}'),\n",
    "            'citation_numeric': re.compile(r'\\[(\\d+(?:,\\s*\\d+)*)\\]'),\n",
    "            'citation_author': re.compile(r'\\([A-Z][a-z]+(?:\\s+et al\\.?)?,?\\s+\\d{4}\\)'),\n",
    "            'url': re.compile(r'https?://[^\\s]+'),\n",
    "            'doi': re.compile(r'doi:\\s*[^\\s]+', re.IGNORECASE),\n",
    "            'number_unit': re.compile(r'\\b\\d+\\.?\\d*\\s*(?:mm|cm|m|km|mg|g|kg|ml|l|¬∞C|K|Hz|kHz|MHz|GHz|V|mV|A|mA|W|Pa|MPa|mol|%)\\b'),\n",
    "        }\n",
    "        self.placeholder_map = {}\n",
    "        self.counter = {}\n",
    "    \n",
    "    def protect(self, text: str):\n",
    "        \"\"\"Protect special tokens with placeholders\"\"\"\n",
    "        protected = text\n",
    "        self.placeholder_map = {}\n",
    "        self.counter = {}\n",
    "        \n",
    "        for name in ['latex_env', 'math_display', 'code_block', 'math_inline', \n",
    "                     'code_inline', 'citation_latex', 'citation_author', \n",
    "                     'citation_numeric', 'doi', 'url', 'number_unit']:\n",
    "            pattern = self.patterns[name]\n",
    "            protected = self._replace(protected, pattern, name.upper())\n",
    "        \n",
    "        return protected, self.placeholder_map\n",
    "    \n",
    "    def _replace(self, text, pattern, token_type):\n",
    "        \"\"\"Replace matches with placeholders\"\"\"\n",
    "        def repl(match):\n",
    "            if token_type not in self.counter:\n",
    "                self.counter[token_type] = 0\n",
    "            self.counter[token_type] += 1\n",
    "            placeholder = f\"<{token_type}_{self.counter[token_type]}>\"\n",
    "            self.placeholder_map[placeholder] = match.group(0)\n",
    "            return placeholder\n",
    "        return pattern.sub(repl, text)\n",
    "    \n",
    "    def restore(self, text, placeholder_map):\n",
    "        \"\"\"Restore protected tokens\"\"\"\n",
    "        for placeholder, original in placeholder_map.items():\n",
    "            text = text.replace(placeholder, original)\n",
    "        return text\n",
    "\n",
    "protector = AdvancedTokenProtector()\n",
    "print(\"‚úÖ Advanced token protector loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:39:02.952940Z",
     "iopub.status.busy": "2026-02-03T22:39:02.952666Z",
     "iopub.status.idle": "2026-02-03T22:39:02.968382Z",
     "shell.execute_reply": "2026-02-03T22:39:02.967677Z",
     "shell.execute_reply.started": "2026-02-03T22:39:02.952910Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quality analyzer loaded!\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Quality Analyzer\n",
    "\n",
    "class QualityAnalyzer:\n",
    "    \"\"\"Analyze paraphrase quality with detailed metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def compute_lexical_overlap(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate word overlap percentage\"\"\"\n",
    "        tokens1 = set(re.findall(r'\\w+', text1.lower()))\n",
    "        tokens2 = set(re.findall(r'\\w+', text2.lower()))\n",
    "        if not tokens1:\n",
    "            return 0.0\n",
    "        return len(tokens1 & tokens2) / len(tokens1)\n",
    "    \n",
    "    def compute_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate meaning similarity using AI embeddings\"\"\"\n",
    "        emb = self.embedding_model.encode([text1, text2], convert_to_tensor=True)\n",
    "        return util.cos_sim(emb[0], emb[1]).item()\n",
    "    \n",
    "    def analyze(self, original: str, paraphrased: str) -> Dict:\n",
    "        \"\"\"Complete quality analysis\"\"\"\n",
    "        lex = self.compute_lexical_overlap(original, paraphrased)\n",
    "        sem = self.compute_semantic_similarity(original, paraphrased)\n",
    "        \n",
    "        return {\n",
    "            'lexical_overlap': lex,\n",
    "            'semantic_similarity': sem,\n",
    "            'is_excellent': (0.30 <= lex <= 0.55 and 0.78 <= sem <= 0.92),\n",
    "            'status': self._get_status(lex, sem)\n",
    "        }\n",
    "    \n",
    "    def _get_status(self, lex, sem):\n",
    "        \"\"\"Determine quality status\"\"\"\n",
    "        if 0.30 <= lex <= 0.55 and 0.78 <= sem <= 0.92:\n",
    "            return \"‚úÖ EXCELLENT\"\n",
    "        elif lex > 0.65:\n",
    "            return \"‚ö†Ô∏è TOO_SIMILAR\"\n",
    "        elif sem < 0.70:\n",
    "            return \"‚ö†Ô∏è MEANING_DRIFT\"\n",
    "        elif sem > 0.95:\n",
    "            return \"‚ö†Ô∏è NEARLY_IDENTICAL\"\n",
    "        else:\n",
    "            return \"‚úÖ GOOD\"\n",
    "\n",
    "analyzer = QualityAnalyzer(embedding_model)\n",
    "print(\"‚úÖ Quality analyzer loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:39:02.969534Z",
     "iopub.status.busy": "2026-02-03T22:39:02.969303Z",
     "iopub.status.idle": "2026-02-03T22:39:02.986497Z",
     "shell.execute_reply": "2026-02-03T22:39:02.985633Z",
     "shell.execute_reply.started": "2026-02-03T22:39:02.969513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Paraphraser ready with Llama 3.3 70B (70 billion parameters)!\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Large Language Model Paraphraser (Llama 3.3 70B)\n",
    "\n",
    "class LLMParaphraser:\n",
    "    \"\"\"Advanced paraphrasing using Groq's Llama 3.3 70B\"\"\"\n",
    "    \n",
    "    def __init__(self, groq_client, protector, analyzer):\n",
    "        self.client = groq_client\n",
    "        self.protector = protector\n",
    "        self.analyzer = analyzer\n",
    "        self.model = \"llama-3.3-70b-versatile\"  # 70 billion parameter model\n",
    "    \n",
    "    def paraphrase(self, text: str, sources: List[str] = None) -> Dict:\n",
    "        \"\"\"Generate 3 high-quality paraphrases\"\"\"\n",
    "        \n",
    "        print(\"=\"*90)\n",
    "        print(\"üéì ADVANCED PARAPHRASING TOOL - POWERED BY LLAMA 3.3 70B\")\n",
    "        print(\"=\"*90)\n",
    "        print(f\"\\nüìÑ ORIGINAL TEXT ({len(text)} characters):\")\n",
    "        print(\"-\"*90)\n",
    "        print(text)\n",
    "        print(\"-\"*90)\n",
    "        \n",
    "        if sources:\n",
    "            print(f\"\\nüìö SOURCES ({len(sources)}):\")\n",
    "            for i, src in enumerate(sources, 1):\n",
    "                print(f\"   [{i}] {src}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Remember to cite your sources!\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"üîÑ GENERATING 3 PARAPHRASES (This may take 10-15 seconds)...\")\n",
    "        print(\"=\"*90)\n",
    "        \n",
    "        # Protect technical elements\n",
    "        protected, placeholder_map = self.protector.protect(text)\n",
    "        \n",
    "        # Define 3 prompting strategies\n",
    "        prompts = {\n",
    "            \"Option 1 - Academic Restructure\": self._create_prompt(protected, \"academic\"),\n",
    "            \"Option 2 - Concise Professional\": self._create_prompt(protected, \"concise\"),\n",
    "            \"Option 3 - Technical Elaboration\": self._create_prompt(protected, \"technical\")\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for i, (style, prompt) in enumerate(prompts.items(), 1):\n",
    "            print(f\"\\n‚è≥ {i}/3 - Generating {style}...\")\n",
    "            \n",
    "            try:\n",
    "                # Call Groq API\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\", \n",
    "                            \"content\": \"You are an expert academic writer. Paraphrase scientific text while preserving exact meaning and technical accuracy. Output ONLY the paraphrased text, no explanations.\"\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=1024,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "                \n",
    "                # Extract and clean\n",
    "                paraphrased = response.choices[0].message.content.strip()\n",
    "                style_type = \"academic\" if \"Academic\" in style else \"concise\" if \"Concise\" in style else \"technical\"\n",
    "                paraphrased = self._clean_output(paraphrased, style_type)\n",
    "                restored = self.protector.restore(paraphrased, placeholder_map)\n",
    "                \n",
    "                # Analyze quality\n",
    "                quality = self.analyzer.analyze(text, restored)\n",
    "                # Assess humanness\n",
    "                humanness = humanizer.assess_humanness(restored)\n",
    "                results[f\"Option {i}\"] = {\n",
    "                    'style': style,\n",
    "                    'text': restored,\n",
    "                    'lexical_overlap': quality['lexical_overlap'],\n",
    "                    'semantic_similarity': quality['semantic_similarity'],\n",
    "                    'quality_status': quality['status'],\n",
    "                    'humanness_score': humanness['humanness_score'],\n",
    "                    'humanness_status': humanness['status']\n",
    "                }\n",
    "                \n",
    "                print(f\"   ‚úÖ Done!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "                results[f\"Option {i}\"] = {\n",
    "                    'style': style,\n",
    "                    'text': f\"Error: {str(e)}\",\n",
    "                    'lexical_overlap': 0.0,\n",
    "                    'semantic_similarity': 0.0,\n",
    "                    'quality_status': \"ERROR\"\n",
    "                }\n",
    "        \n",
    "        # Display results\n",
    "        self._display_results(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _create_prompt(self, text: str, style: str) -> str:\n",
    "        \"\"\"Create style-specific prompts\"\"\"\n",
    "        \n",
    "        base = \"\"\"Paraphrase this text while:\n",
    "1. Preserving EXACT original meaning\n",
    "2. Maintaining technical accuracy\n",
    "3. Keeping ALL placeholders unchanged (like <CITATION_1>, <URL_1>)\n",
    "4. Using different sentence structures and vocabulary\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        styles = {\n",
    "            \"academic\": \"Style: Academic restructure - Rearrange sentences, scholarly language, formal tone.\",\n",
    "            \"concise\": \"Style: Concise professional - Reduce wordiness, direct statements, clear and brief.\",\n",
    "            \"technical\": \"Style: Technical elaboration - Emphasize technical details, add clarity, maintain precision.\"\n",
    "        }\n",
    "        \n",
    "        return base + styles[style] + f\"\\n\\nText:\\n{text}\\n\\nParaphrase:\"\n",
    "    \n",
    "    def _clean_output(self, text: str, style: str = \"academic\") -> str:\n",
    "        \n",
    "        \"\"\"Clean and HUMANIZE LLM output\"\"\"\n",
    "        # Basic cleaning\n",
    "        text = re.sub(r'^(Paraphrase:|Here is|Here\\'s|The paraphrased).*?:\\s*', '', text, flags=re.IGNORECASE)\n",
    "        text = text.strip()\n",
    "    \n",
    "        # APPLY HUMANIZATION (NEW!)\n",
    "        text = humanizer.humanize(text, style)\n",
    "    \n",
    "        return text\n",
    "\n",
    "    \n",
    "    def _display_results(self, results: Dict):\n",
    "        \"\"\"Display formatted results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"‚úÖ PARAPHRASING COMPLETE!\")\n",
    "        print(\"=\"*90)\n",
    "        \n",
    "        for option, data in results.items():\n",
    "            print(f\"\\n{'='*90}\")\n",
    "            print(f\"üìù {option.upper()}: {data['style']}\")\n",
    "            print('='*90)\n",
    "            print(data['text'])\n",
    "            print(f\"\\nüìä Quality Metrics:\")\n",
    "            print(f\"   ‚Ä¢ Lexical Overlap: {data['lexical_overlap']:.1%}\")\n",
    "            print(f\"   ‚Ä¢ Semantic Similarity: {data['semantic_similarity']:.1%}\")\n",
    "            print(f\"   ‚Ä¢ Quality Status: {data['quality_status']}\")\n",
    "            print(f\"   ‚Ä¢ Humanness Score: {data['humanness_score']:.0f}/100 ({data['humanness_status']})\")\n",
    "            print()\n",
    "        \n",
    "        print(\"=\"*90)\n",
    "        print(\"üí° REMEMBER: Always cite your sources in the manuscript!\")\n",
    "        print(\"=\"*90)\n",
    "\n",
    "# Initialize the paraphraser\n",
    "llm_paraphraser = LLMParaphraser(groq_client, protector, analyzer)\n",
    "print(\"‚úÖ LLM Paraphraser ready with Llama 3.3 70B (70 billion parameters)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:39:02.988027Z",
     "iopub.status.busy": "2026-02-03T22:39:02.987523Z",
     "iopub.status.idle": "2026-02-03T22:39:03.005650Z",
     "shell.execute_reply": "2026-02-03T22:39:03.004943Z",
     "shell.execute_reply.started": "2026-02-03T22:39:02.988001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Humanization processor loaded!\n",
      "   ‚Ä¢ Removes robotic phrases\n",
      "   ‚Ä¢ Improves sentence flow\n",
      "   ‚Ä¢ Enhances naturalness\n"
     ]
    }
   ],
   "source": [
    "# CELL 5.5: Humanization Processor\n",
    "\n",
    "class HumanizationProcessor:\n",
    "    \"\"\"Advanced humanization to make AI text more natural\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Robotic phrases to replace\n",
    "        self.robotic_replacements = {\n",
    "            # Formal ‚Üí Natural\n",
    "            'It is important to note that': 'Notably,',\n",
    "            'It should be noted that': 'Note that',\n",
    "            'It is worth mentioning that': 'Interestingly,',\n",
    "            'In order to': 'To',\n",
    "            'Due to the fact that': 'Because',\n",
    "            'In light of the fact that': 'Since',\n",
    "            'For the purpose of': 'To',\n",
    "            'With regard to': 'Regarding',\n",
    "            'In the event that': 'If',\n",
    "            'At this point in time': 'Now',\n",
    "            \n",
    "            # Academic stiffness ‚Üí Flow\n",
    "            'It is evident that': 'Clearly,',\n",
    "            'It can be observed that': 'Observations show',\n",
    "            'It has been shown that': 'Research shows',\n",
    "            'It is well established that': 'Studies confirm',\n",
    "            \n",
    "            # Passive ‚Üí Active voice improvements\n",
    "            'are characterized by': 'exhibit',\n",
    "            'are composed of': 'consist of',\n",
    "            'are associated with': 'relate to',\n",
    "            'are utilized': 'use',\n",
    "            'is utilized': 'uses',\n",
    "            \n",
    "            # Wordiness ‚Üí Conciseness\n",
    "            'a majority of': 'most',\n",
    "            'a number of': 'several',\n",
    "            'at the present time': 'currently',\n",
    "            'during the course of': 'during',\n",
    "            'in the absence of': 'without',\n",
    "        }\n",
    "        \n",
    "        # AI-like phrases to avoid\n",
    "        self.ai_tells = [\n",
    "            'as an AI',\n",
    "            'I cannot',\n",
    "            'I am not able to',\n",
    "            'based on my training',\n",
    "            'here is the paraphrase',\n",
    "            'here\\'s the paraphrase',\n",
    "            'the paraphrased version',\n",
    "        ]\n",
    "    \n",
    "    def humanize(self, text: str, style: str = \"academic\") -> str:\n",
    "        \"\"\"Apply humanization transformations\"\"\"\n",
    "        \n",
    "        # Step 1: Remove AI artifacts\n",
    "        text = self._remove_ai_artifacts(text)\n",
    "        \n",
    "        # Step 2: Replace robotic phrases\n",
    "        text = self._replace_robotic_phrases(text)\n",
    "        \n",
    "        # Step 3: Improve flow\n",
    "        text = self._improve_flow(text)\n",
    "        \n",
    "        # Step 4: Style-specific adjustments\n",
    "        text = self._apply_style_adjustments(text, style)\n",
    "        \n",
    "        # Step 5: Final polish\n",
    "        text = self._final_polish(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _remove_ai_artifacts(self, text: str) -> str:\n",
    "        \"\"\"Remove obvious AI-generated artifacts\"\"\"\n",
    "        for phrase in self.ai_tells:\n",
    "            text = re.sub(phrase, '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove meta-commentary\n",
    "        text = re.sub(r'^(Paraphrase:|Here is|Here\\'s|The following|Below is).*?:\\s*', \n",
    "                     '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def _replace_robotic_phrases(self, text: str) -> str:\n",
    "        \"\"\"Replace stiff academic phrases\"\"\"\n",
    "        for robotic, natural in self.robotic_replacements.items():\n",
    "            # Case-insensitive replacement\n",
    "            pattern = re.compile(re.escape(robotic), re.IGNORECASE)\n",
    "            text = pattern.sub(natural, text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _improve_flow(self, text: str) -> str:\n",
    "        \"\"\"Improve sentence flow and transitions\"\"\"\n",
    "        \n",
    "        # Add variety to sentence starts\n",
    "        sentences = text.split('. ')\n",
    "        \n",
    "        # Check for repetitive starts\n",
    "        starts = [s.split()[0] if s.split() else '' for s in sentences]\n",
    "        \n",
    "        # If too many sentences start with \"The\", vary them\n",
    "        if starts.count('The') > len(sentences) * 0.4:\n",
    "            for i, s in enumerate(sentences):\n",
    "                if s.startswith('The ') and i > 0:\n",
    "                    # Add transition words occasionally\n",
    "                    transitions = ['Additionally,', 'Furthermore,', 'Moreover,', '']\n",
    "                    if i % 2 == 0:\n",
    "                        sentences[i] = transitions[i % len(transitions)] + ' ' + s if transitions[i % len(transitions)] else s\n",
    "        \n",
    "        text = '. '.join(sentences)\n",
    "        \n",
    "        # Remove double spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _apply_style_adjustments(self, text: str, style: str) -> str:\n",
    "        \"\"\"Apply style-specific humanization\"\"\"\n",
    "        \n",
    "        if style == \"concise\":\n",
    "            # Remove excessive qualifiers\n",
    "            text = re.sub(r'\\b(very|extremely|highly|quite|rather)\\s+', '', text)\n",
    "            \n",
    "            # Simplify complex structures\n",
    "            text = text.replace('in which', 'where')\n",
    "            text = text.replace('that which', 'what')\n",
    "            \n",
    "        elif style == \"technical\":\n",
    "            # Ensure technical terms are precise\n",
    "            # Keep technical language but make connectors natural\n",
    "            text = text.replace('thus,', 'therefore,')\n",
    "            text = text.replace('hence,', 'consequently,')\n",
    "            \n",
    "        elif style == \"academic\":\n",
    "            # Balance formality with readability\n",
    "            # Keep scholarly tone but improve flow\n",
    "            pass\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _final_polish(self, text: str) -> str:\n",
    "        \"\"\"Final polish for natural output\"\"\"\n",
    "        \n",
    "        # Ensure proper capitalization\n",
    "        if text and text[0].islower():\n",
    "            text = text[0].upper() + text[1:]\n",
    "        \n",
    "        # Ensure proper ending punctuation\n",
    "        if text and text[-1] not in '.!?':\n",
    "            text += '.'\n",
    "        \n",
    "        # Fix spacing around punctuation\n",
    "        text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "        text = re.sub(r'([.,;:!?])(\\w)', r'\\1 \\2', text)\n",
    "        \n",
    "        # Remove multiple punctuation\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def assess_humanness(self, text: str) -> Dict:\n",
    "        \"\"\"Assess how human-like the text is\"\"\"\n",
    "        \n",
    "        # Check for AI tells\n",
    "        ai_markers = sum(1 for phrase in self.ai_tells if phrase.lower() in text.lower())\n",
    "        \n",
    "        # Check for robotic phrases\n",
    "        robotic_count = sum(1 for phrase in self.robotic_replacements.keys() \n",
    "                           if phrase.lower() in text.lower())\n",
    "        \n",
    "        # Check sentence variety\n",
    "        sentences = text.split('.')\n",
    "        sentence_starts = [s.strip().split()[0] if s.strip().split() else '' \n",
    "                          for s in sentences if s.strip()]\n",
    "        \n",
    "        variety_score = len(set(sentence_starts)) / max(len(sentence_starts), 1)\n",
    "        \n",
    "        # Calculate humanness score (0-100)\n",
    "        humanness = 100\n",
    "        humanness -= ai_markers * 20  # -20 per AI tell\n",
    "        humanness -= robotic_count * 5  # -5 per robotic phrase\n",
    "        humanness += variety_score * 10  # +10 for variety\n",
    "        \n",
    "        humanness = max(0, min(100, humanness))\n",
    "        \n",
    "        return {\n",
    "            'humanness_score': humanness,\n",
    "            'ai_markers': ai_markers,\n",
    "            'robotic_phrases': robotic_count,\n",
    "            'sentence_variety': variety_score,\n",
    "            'status': 'EXCELLENT' if humanness >= 80 else 'GOOD' if humanness >= 60 else 'NEEDS_WORK'\n",
    "        }\n",
    "\n",
    "humanizer = HumanizationProcessor()\n",
    "print(\"‚úÖ Humanization processor loaded!\")\n",
    "print(\"   ‚Ä¢ Removes robotic phrases\")\n",
    "print(\"   ‚Ä¢ Improves sentence flow\")\n",
    "print(\"   ‚Ä¢ Enhances naturalness\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:39:03.007026Z",
     "iopub.status.busy": "2026-02-03T22:39:03.006711Z",
     "iopub.status.idle": "2026-02-03T22:39:03.018831Z",
     "shell.execute_reply": "2026-02-03T22:39:03.018224Z",
     "shell.execute_reply.started": "2026-02-03T22:39:03.007008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Simple interface loaded!\n",
      "\n",
      "======================================================================\n",
      "üìñ USAGE INSTRUCTIONS:\n",
      "======================================================================\n",
      "1. results = paraphrase(\"Your text...\", [\"Source\"])\n",
      "2. option1 = results[\"Option 1\"]['text']\n",
      "3. option2 = results[\"Option 2\"]['text']\n",
      "4. option3 = results[\"Option 3\"]['text']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Simple Interface Function\n",
    "\n",
    "def paraphrase(text, sources=None):\n",
    "    \"\"\"\n",
    "    Simple paraphrasing interface\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to paraphrase\n",
    "        sources (list): List of citation sources (optional but recommended)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with 3 paraphrased options\n",
    "    \n",
    "    Example:\n",
    "        results = paraphrase(\"Your text here\", [\"Source 1\", \"Source 2\"])\n",
    "        option1 = results[\"Option 1\"]['text']\n",
    "    \"\"\"\n",
    "    return llm_paraphraser.paraphrase(text, sources)\n",
    "\n",
    "print(\"‚úÖ Simple interface loaded!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìñ USAGE INSTRUCTIONS:\")\n",
    "print(\"=\"*70)\n",
    "print('1. results = paraphrase(\"Your text...\", [\"Source\"])')\n",
    "print('2. option1 = results[\"Option 1\"][\\'text\\']')\n",
    "print('3. option2 = results[\"Option 2\"][\\'text\\']')\n",
    "print('4. option3 = results[\"Option 3\"][\\'text\\']')\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:39:03.019969Z",
     "iopub.status.busy": "2026-02-03T22:39:03.019702Z",
     "iopub.status.idle": "2026-02-03T22:39:03.033161Z",
     "shell.execute_reply": "2026-02-03T22:39:03.032526Z",
     "shell.execute_reply.started": "2026-02-03T22:39:03.019946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Humanization comparison tool loaded!\n",
      "\n",
      "Usage: compare_humanization(\"Your text...\")\n"
     ]
    }
   ],
   "source": [
    "# CELL 6.5: Compare Before/After Humanization\n",
    "\n",
    "def compare_humanization(original_text: str, show_details: bool = True):\n",
    "    \"\"\"\n",
    "    Compare text before and after humanization\n",
    "    \n",
    "    Usage:\n",
    "        compare_humanization(\"Your text here\")\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üî¨ HUMANIZATION COMPARISON TOOL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìÑ ORIGINAL TEXT:\")\n",
    "    print(\"-\"*80)\n",
    "    print(original_text)\n",
    "    \n",
    "    # Assess original\n",
    "    original_score = humanizer.assess_humanness(original_text)\n",
    "    \n",
    "    print(f\"\\nüìä Original Humanness: {original_score['humanness_score']:.0f}/100\")\n",
    "    print(f\"   Status: {original_score['status']}\")\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"   ‚Ä¢ AI Markers: {original_score['ai_markers']}\")\n",
    "        print(f\"   ‚Ä¢ Robotic Phrases: {original_score['robotic_phrases']}\")\n",
    "        print(f\"   ‚Ä¢ Sentence Variety: {original_score['sentence_variety']:.2f}\")\n",
    "    \n",
    "    # Humanize\n",
    "    humanized = humanizer.humanize(original_text, \"academic\")\n",
    "    \n",
    "    print(\"\\n‚ú® HUMANIZED TEXT:\")\n",
    "    print(\"-\"*80)\n",
    "    print(humanized)\n",
    "    \n",
    "    # Assess humanized\n",
    "    humanized_score = humanizer.assess_humanness(humanized)\n",
    "    \n",
    "    print(f\"\\nüìä Humanized Score: {humanized_score['humanness_score']:.0f}/100\")\n",
    "    print(f\"   Status: {humanized_score['status']}\")\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"   ‚Ä¢ AI Markers: {humanized_score['ai_markers']}\")\n",
    "        print(f\"   ‚Ä¢ Robotic Phrases: {humanized_score['robotic_phrases']}\")\n",
    "        print(f\"   ‚Ä¢ Sentence Variety: {humanized_score['sentence_variety']:.2f}\")\n",
    "    \n",
    "    improvement = humanized_score['humanness_score'] - original_score['humanness_score']\n",
    "    \n",
    "    print(\"\\nüìà IMPROVEMENT:\")\n",
    "    print(f\"   {'+' if improvement >= 0 else ''}{improvement:.0f} points\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'original': original_text,\n",
    "        'humanized': humanized,\n",
    "        'original_score': original_score,\n",
    "        'humanized_score': humanized_score,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Humanization comparison tool loaded!\")\n",
    "print(\"\\nUsage: compare_humanization(\\\"Your text...\\\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T22:39:03.035819Z",
     "iopub.status.busy": "2026-02-03T22:39:03.035206Z",
     "iopub.status.idle": "2026-02-03T22:39:06.067932Z",
     "shell.execute_reply": "2026-02-03T22:39:06.067116Z",
     "shell.execute_reply.started": "2026-02-03T22:39:03.035790Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "üéì ADVANCED PARAPHRASING TOOL - POWERED BY LLAMA 3.3 70B\n",
      "==========================================================================================\n",
      "\n",
      "üìÑ ORIGINAL TEXT (650 characters):\n",
      "------------------------------------------------------------------------------------------\n",
      "DL has revolutionized cancer genomics by enhancing diagnostic accuracy and enabling personalized medicine through the development of advanced computational models. These systems integrate genomic data with other diagnostic tools, such as radiomics and pathology imaging, to create a more comprehensive framework for cancer detection, thereby improving clinical decision making. One key challenge in genomic analysis is the presence of imbalanced data sets, which can lead to biased predictions. To address this, methods like SMOTE-Tomek resampling help balance training data, making DL models more robust and generalizable across patient populations.\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "‚ö†Ô∏è  Remember to cite your sources!\n",
      "\n",
      "==========================================================================================\n",
      "üîÑ GENERATING 3 PARAPHRASES (This may take 10-15 seconds)...\n",
      "==========================================================================================\n",
      "\n",
      "‚è≥ 1/3 - Generating Option 1 - Academic Restructure...\n",
      "   ‚úÖ Done!\n",
      "\n",
      "‚è≥ 2/3 - Generating Option 2 - Concise Professional...\n",
      "   ‚úÖ Done!\n",
      "\n",
      "‚è≥ 3/3 - Generating Option 3 - Technical Elaboration...\n",
      "   ‚úÖ Done!\n",
      "\n",
      "==========================================================================================\n",
      "‚úÖ PARAPHRASING COMPLETE!\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "üìù OPTION 1: Option 1 - Academic Restructure\n",
      "==========================================================================================\n",
      "The advent of deep learning (DL) has transformed the field of cancer genomics, yielding enhanced diagnostic precision and facilitating the implementation of personalized medicine via the creation of sophisticated computational frameworks. By consolidating genomic data with supplementary diagnostic modalities, including radiomics and pathology imaging, these advanced systems establish a more holistic paradigm for cancer detection, ultimately augmenting clinical decision-making processes. A significant obstacle in the analysis of genomic data is the prevalence of class imbalance, which can result in skewed predictive outcomes. Techniques such as SMOTE-Tomek resampling have been employed to mitigate this issue, thereby enhancing the robustness and generalizability of DL models across diverse patient cohorts, as discussed in <CITATION_1> and accessible at <URL_1>.\n",
      "\n",
      "üìä Quality Metrics:\n",
      "   ‚Ä¢ Lexical Overlap: 60.0%\n",
      "   ‚Ä¢ Semantic Similarity: 87.2%\n",
      "   ‚Ä¢ Quality Status: ‚úÖ GOOD\n",
      "   ‚Ä¢ Humanness Score: 100/100 (EXCELLENT)\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "üìù OPTION 2: Option 2 - Concise Professional\n",
      "==========================================================================================\n",
      "The application of DL in cancer genomics has significantly enhanced diagnostic precision and facilitated personalized treatment approaches by leveraging sophisticated computational models. By combining genomic information with radiomics and pathology imaging, these systems establish a more integrated framework for cancer identification, ultimately refining clinical decision-making processes. A major obstacle in genomic analysis is the issue of imbalanced datasets, which can result in skewed predictions. Techniques such as SMOTE-Tomek resampling have been employed to balance training data, thereby increasing the robustness and generalizability of DL models across diverse patient populations.\n",
      "\n",
      "üìä Quality Metrics:\n",
      "   ‚Ä¢ Lexical Overlap: 61.3%\n",
      "   ‚Ä¢ Semantic Similarity: 97.4%\n",
      "   ‚Ä¢ Quality Status: ‚ö†Ô∏è NEARLY_IDENTICAL\n",
      "   ‚Ä¢ Humanness Score: 100/100 (EXCELLENT)\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "üìù OPTION 3: Option 3 - Technical Elaboration\n",
      "==========================================================================================\n",
      "The advent of deep learning (DL) has transformed the field of cancer genomics by significantly augmenting diagnostic precision and facilitating tailored therapeutic approaches through the creation of sophisticated computational architectures. By amalgamating genomic information with complementary diagnostic modalities, including radiomics and pathology imaging, these systems establish a more exhaustive paradigm for cancer identification, ultimately enhancing the efficacy of clinical decision-making processes. A pivotal obstacle in the analysis of genomic data is the prevalence of class-imbalanced datasets, which can yield skewed predictive outcomes. To mitigate this issue, techniques such as SMOTE-Tomek resampling are employed to establish equilibrium in training datasets, thereby rendering DL models more resilient and adaptable across diverse patient cohorts.\n",
      "\n",
      "üìä Quality Metrics:\n",
      "   ‚Ä¢ Lexical Overlap: 58.7%\n",
      "   ‚Ä¢ Semantic Similarity: 88.3%\n",
      "   ‚Ä¢ Quality Status: ‚úÖ GOOD\n",
      "   ‚Ä¢ Humanness Score: 100/100 (EXCELLENT)\n",
      "\n",
      "==========================================================================================\n",
      "üí° REMEMBER: Always cite your sources in the manuscript!\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "üíæ PARAPHRASES SAVED TO VARIABLES\n",
      "==========================================================================================\n",
      "‚Ä¢ option1_text - Academic Restructure\n",
      "‚Ä¢ option2_text - Concise Professional\n",
      "‚Ä¢ option3_text - Technical Elaboration\n",
      "==========================================================================================\n",
      "\n",
      "üí° TIP: Copy the best paraphrase for your manuscript, then CITE YOUR SOURCES!\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Run Paraphrasing\n",
    "\n",
    "# ============================================================================\n",
    "# YOUR TEXT HERE - Replace with any paragraph you want to paraphrase\n",
    "# ============================================================================\n",
    "\n",
    "my_text = \"\"\"DL has revolutionized cancer genomics by enhancing diagnostic accuracy and enabling personalized medicine through the development of advanced computational models. These systems integrate genomic data with other diagnostic tools, such as radiomics and pathology imaging, to create a more comprehensive framework for cancer detection, thereby improving clinical decision making. One key challenge in genomic analysis is the presence of imbalanced data sets, which can lead to biased predictions. To address this, methods like SMOTE-Tomek resampling help balance training data, making DL models more robust and generalizable across patient populations.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# YOUR SOURCES HERE - Replace with your actual citations\n",
    "# ============================================================================\n",
    "\n",
    "my_sources = [\n",
    "   \n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE PARAPHRASES\n",
    "# ============================================================================\n",
    "\n",
    "results = paraphrase(my_text, my_sources)\n",
    "\n",
    "# ============================================================================\n",
    "# ACCESS INDIVIDUAL PARAPHRASES\n",
    "# ============================================================================\n",
    "\n",
    "option1_text = results[\"Option 1\"]['text']\n",
    "option2_text = results[\"Option 2\"]['text']\n",
    "option3_text = results[\"Option 3\"]['text']\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üíæ PARAPHRASES SAVED TO VARIABLES\")\n",
    "print(\"=\"*90)\n",
    "print(\"‚Ä¢ option1_text - Academic Restructure\")\n",
    "print(\"‚Ä¢ option2_text - Concise Professional\")\n",
    "print(\"‚Ä¢ option3_text - Technical Elaboration\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\nüí° TIP: Copy the best paraphrase for your manuscript, then CITE YOUR SOURCES!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRICT RESTRICTED LICENSE (SRL-1.0)**\n",
    "\n",
    "**Copyright (c) 2026 Argha Mukherjee**\n",
    "\n",
    "**Permission Notice**\n",
    "\n",
    "1.  **Grant of Rights:** 1.1 Subject to the terms below, the copyright holder **Argha Mukherjee** (‚ÄúCopyright Holder‚Äù) grants to any person or entity a non-exclusive, worldwide license to use, reproduce, and distribute the Software (defined below). 1.2 The license granted under Section 1.1 is conditional. Additional obligations apply if the recipient modifies the Software or uses the Software for commercial purposes as set out in Sections 2 and 3.\n",
    "    \n",
    "2.  **Definitions:**\n",
    "    \n",
    "    *   ‚ÄúSoftware‚Äù means the source code, object code, documentation, examples, and other material distributed by the Copyright Holder under this license.\n",
    "        \n",
    "    *   ‚ÄúModify‚Äù or ‚ÄúModification‚Äù means any change, adaptation, translation, translation into another programming language, compilation, patch, removal, or derivative work based on the Software.\n",
    "        \n",
    "    *   ‚ÄúCommercial Use‚Äù means any use of the Software that results in revenue, including but not limited to sale, licensing, sublicensing, renting, subscription services, hosting services, inclusion in paid products or services, or any activity intended for monetary gain.\n",
    "        \n",
    "    *   ‚ÄúGross Revenue‚Äù means the total amounts received by the licensee (and its affiliates) from any Commercial Use directly attributable to the Software, before any deductions for costs, taxes, refunds, credits, or expenses.\n",
    "        \n",
    "3.  **Modifications and Attribution (required):** 3.1 Modifications are permitted only on the following conditions:a) Full Credit ‚Äî Any modified version, derivative work, or work that contains portions of the Software must include a conspicuous attribution crediting the Copyright Holder as follows:‚ÄúPortions ¬© 2026 Argha Mukherjee. Original Software licensed under SRL-1.0.‚ÄùThe attribution must be included in:- a NOTICE or README file distributed with the software,- any about or credits screen for user-facing products,- prominent documentation and product websites where the product is described.b) Modification Notice ‚Äî All modified source files must contain a header comment describing what was changed, the author of the modification, and the date of modification.c) No Removal ‚Äî The original copyright notice and this license text must be preserved in all copies and distributed forms.\n",
    "    \n",
    "4.  **Commercial Use, Payment, and Reporting (mandatory):** 4.1 Commercial Permission ‚Äî Commercial Use of the Software is permitted only after (a) providing written notice to the Copyright Holder at the contact email below, and (b) complying with the payment and reporting obligations in this Section 4.4.2 Revenue Share ‚Äî The licensee must pay the Copyright Holder **a minimum of fifty percent (50%) of Gross Revenue** derived from any Commercial Use that incorporates, is based on, or benefits from the Software.4.3 Payment Schedule and Reports:a) Payment Frequency ‚Äî Payments of the revenue share are due quarterly within thirty (30) days after the end of each calendar quarter.b) Reporting ‚Äî With each payment the licensee must deliver a written report that shows how Gross Revenue was calculated, the relevant sales/transaction records, and the computation supporting the payment.4.4 Audit Right ‚Äî The Copyright Holder (or an independent auditor chosen by the Copyright Holder) may, upon reasonable notice and no more than once each calendar year, inspect relevant financial records of the licensee to verify Gross Revenue. If an audit reveals underpayment by more than five percent (5%), the licensee will reimburse the cost of the audit.4.5 Interest and Remedies ‚Äî Late payments will accrue interest at the lesser of (a) 1.5% per month, or (b) the maximum rate permitted by applicable law. Nonpayment or material breach of these payment terms entitles the Copyright Holder to injunctive relief, termination of this license, and recovery of damages.\n",
    "    \n",
    "5.  **Redistribution:** 5.1 Unmodified redistribution of the Software (source or binary) is permitted provided this license and the copyright and attribution notices are preserved and no fee is charged for the Software itself.5.2 Redistribution that constitutes Commercial Use (see Section 3 and 4) triggers the payment obligations in Section 4.\n",
    "    \n",
    "6.  **Warranties, Liability, and Indemnity:** 6.1 THE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED.6.2 IN NO EVENT SHALL THE COPYRIGHT HOLDER BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES.6.3 The licensee agrees to indemnify and hold harmless the Copyright Holder from third-party claims arising from the licensee‚Äôs Commercial Use or modifications of the Software.\n",
    "    \n",
    "7.  **Termination:** 7.1 This license terminates automatically if the licensee fails to comply with any material term of the license (including payment obligations and attribution requirements).7.2 Termination does not relieve the licensee of any accrued payment obligations or liabilities that arose prior to termination.\n",
    "    \n",
    "8.  **Governing Law and Jurisdiction:** 8.1 This license is governed by the laws of India. The parties submit to the exclusive jurisdiction of the courts located in Kolkata, India for resolution of any disputes, unless otherwise agreed in writing.\n",
    "    \n",
    "9.  **Severability:** 9.1 If any provision of this license is found invalid or unenforceable, the remainder of the license remains in force to the fullest extent permitted by law.\n",
    "    \n",
    "10.  **Contact** for Permissions, Reporting, and Payment\n",
    "    \n",
    "*   **Email: (arghamukherjee1998@gmail.com)**\n",
    "    \n",
    "\n",
    "11.  **Acceptance:**\n",
    "    11.1 By using, modifying, or distributing the Software, the licensee agrees to be bound by the terms of this license."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
